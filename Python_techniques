QUICK SORTING

#The last element will be taken as a pivot by the use of the function
#The smaller element is placed left to the pivot
#The greater element is placed to the right of the pivot
def partition(array,low,high):
    i = ( low-1 )         # index of smaller element is chosen
    pivot = array[high]     # pivot is chosen

    for j in range(low , high):

        #Is the element less or equal to the pivot
        if   array[j] <= pivot:

            # increment index of smaller element
            i = i+1
            array[i],array[j] = array[j],array[i]

    array[i+1],array[high] = array[high],array[i+1]
    return ( i+1 )

# The main crux of the problem that implements Quick sort is
#array[] is to be sorted
#high is the ending index
#low is the starting index

# Function to do Quick sort
def quickSort(array,low,high):
    if low < high:

       #pit is the partitioning index
        pit = partition(array,low,high)

      #Element sorted before and after partition
        quickSort(array, low, pit-1)
        quickSort(array, pit+1, high)

array=[2,4,6,8,10,12]
n = len(array)
quickSort(array,0,n-1)
print ("The Sorted array is:")
for i in range(n):
    print ("%d" %array[i]),

Dynamic Programming

Data Structures

Hash Tables



####OUTLIERS

#Method 1 STD
import numpy as np
import matplotlib.pyplot as plt
seed(1)
anamalies = []

#multiple and add by random numbers to get real values
random_data = np.random.randn(50000) * 20+20

#Function to detection outlier on one-dimentional datasets
def find_anomalies(data):
    #set upper and lower limit to 3 STD
    random_data_std = std(random_data)
    random_data_mean = mean(random_data)
    anomaly_cut_off=random_data_std * 3

    lower_limit=random_data_mean - anomaly_cut_off
    upper_limit=random_data_mean+anomaly_cut_off
    print(lower_limit)
    #Generate outliers
    for outier in random_data:
        if outlier > upper_limit or outlier < lower_limit:
            anomalies.append(outlier)
    return anomalies

find_anomalies(random_data)

#Method 2 Boxplots
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(data=random_data)

#Method 3 DBScan Clustering (clustering algorithm)
#three important concepts
#1. Core points: 1. min_samples, number of core points needed in order to form a cluster. 2. eps, maximum distance between two samples for them to be considered as in the same cluster.
#2. Border points: same cluster as core points, but farer. noise points are data points that do not belong to any cluster. 
from sklearn.cluster import DBSCAN
seed(1)
random_data = np.random.randn(50000,2) * 20 + 20

outlier_detection = DBSCAN(min_samples = 2, eps = 3)
clusters = outlier_detection.fit_prediction(random_data)
list(clusters).count(-1)

#Method 4-Isolation Forest: unsupervised learning algorithm in decision trees family. Works in high dimensional datasets and it proved to be a very effective way of detecting anomalies. 
from sklearn.ensemble import IsolationForest
import numpy as np
np.random.seed(1)
random_data=np.random.randn(50000,2)*20+20

clf=IsolationForest(behaviour='new',max_samples=100, random_state=1, contamination= 'auto')
preds=clf.fit_predict(random_data)
preds
#results: -1 is an outlier. 1 is not an outlier

#Method 5-Robust Random Cut Forest: Amazon unsupervised algorith to detect anomalies. Low scores = "normal". High scores = anomaly
#https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/random_cut_forest

